{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcadf2f-e3e6-4b02-a6b5-d5f335e23c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "from torch import nn\n",
    "from pl_bolts.models.autoencoders import AE\n",
    "from fastai.vision.all import *\n",
    "from PIL import Image, ImageFile\n",
    "\n",
    "from autoencoder import AESigmoid\n",
    "from classifier import Classifier\n",
    "from tripletnet import TripletNet\n",
    "\n",
    "from lucent.optvis import render, param, transform, objectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b05be5d-e613-44e8-9c2e-2352435c47fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1027281f-093b-45a4-94d3-711516ba8c08",
   "metadata": {},
   "source": [
    "# Visualise activations of trained networks\n",
    "\n",
    "A notebook to visualise optimised activations for the most variable channels in the feature extractor layer of three trained networks and put them together with examples of images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c09388e-dac2-4439-acf9-9de11cb48cf6",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Just need to set the image height used during training, and load the genera of the training examples for the classifier network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708d5b38-70f4-46c9-b8ba-88fae6b64ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_height = 256\n",
    "label_data = \"../data/herbarium-2021-fgvc8-sampled/sample-metadata.csv\"\n",
    "\n",
    "genera = (\n",
    "    pd.read_csv(label_data)\n",
    "      .assign(image_id=lambda df: df.image_id.astype(str))\n",
    "      .set_index(\"image_id\")\n",
    "      .genus\n",
    ")\n",
    "\n",
    "n_genera = genera.unique().shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6be88d9-19d2-4b55-992d-8c0d11f32cb6",
   "metadata": {},
   "source": [
    "## Load networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57ee2e5-391e-4a97-b013-aabc18ef6521",
   "metadata": {},
   "source": [
    "### Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dfb266-48a1-45f9-ac3e-ec92a913e628",
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_model = AESigmoid(input_height=input_height)\n",
    "ae_model = ae_model.load_from_checkpoint(\"../lightning_logs/resnet18_size-256_ae/version_6/checkpoints/epoch=24-step=331849.ckpt\", \n",
    "                                   strict=False, input_height=input_height)\n",
    "print(\"set up autoencoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95e871b-ce88-45e8-beef-b0fe88887e48",
   "metadata": {},
   "source": [
    "### Triplet network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9a64e0-4de8-4a61-aef3-29477871ccb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = resnet18(pretrained=True)\n",
    "encoder = nn.Sequential(*list(encoder.children())[:-1], nn.Flatten())\n",
    "\n",
    "tpl_model = TripletNet(encoder)\n",
    "tpl_model = tpl_model.load_from_checkpoint(\"../lightning_logs/resnet18_size-256_tpl/version_6/checkpoints/epoch=24-step=331849.ckpt\", \n",
    "                                   strict=False, encoder=encoder)\n",
    "print(\"set up triplet network\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7388b3f4-261e-4af7-bcd0-8860835db6bc",
   "metadata": {},
   "source": [
    "### Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899fe9d1-7884-46b9-b01d-90f14235cef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = resnet18(pretrained=True)\n",
    "encoder = nn.Sequential(*list(encoder.children())[:-1], nn.Flatten())\n",
    "\n",
    "clf_model = Classifier(encoder, 512, genera.unique().shape[0])\n",
    "clf_model = clf_model.load_from_checkpoint(\"../lightning_logs/resnet18_size-256_clf/version_17/checkpoints/epoch=24-step=331849.ckpt\", \n",
    "                                   strict=False, encoder=encoder, input_dim=512, num_classes=genera.unique().shape[0])\n",
    "print(\"set up classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc14c920-df2f-4c73-b3f6-075be850ab3e",
   "metadata": {},
   "source": [
    "## Load features\n",
    "\n",
    "We're using features extracted from the training data here. \n",
    "\n",
    "We're also finding the channel with the greatest variation. There are 512 channels in the feature extraction layer of each network, so looking at all of them would be hard. So we'll just look at the ones with the greatest variation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3077edf5-39f5-4fea-b119-47257150f515",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_values_ae = np.load(\"../output/resnet18_size-256_ae/version_6/features_train.npy\")\n",
    "max_channel_ae = feat_values_ae.std(axis=0).argmax()\n",
    "\n",
    "feat_values_tpl = np.load(\"../output/resnet18_size-256_tpl/version_6/features_train.npy\")\n",
    "max_channel_tpl = feat_values_tpl.std(axis=0).argmax()\n",
    "\n",
    "feat_values_clf = np.load(\"../output/resnet18_size-256_clf/version_17/features_train.npy\")\n",
    "max_channel_clf = feat_values_clf.std(axis=0).argmax()\n",
    "\n",
    "print(f\"Autoencoder channel with greatest variation: {max_channel_ae}\")\n",
    "print(f\"Triplet channel with greatest variation: {max_channel_tpl}\")\n",
    "print(f\"Classifier channel with greatest variation: {max_channel_clf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d126940f-e854-42a0-b26f-91302582e255",
   "metadata": {},
   "source": [
    "## Visualise optimised activations\n",
    "\n",
    "We'll use [lucent](https://github.com/greentfrapp/lucent) to visualise the optimised activations. We're visualising the most positive optimised activation and the most negative, so we can look at images that give a spectrum of activations for each channel.\n",
    "\n",
    "First we need to set our models to evaluation mode and transfer them to the device we want to use. **NB: this may take a long time if you don't have access to a gpu**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c905c4f1-be14-4639-bd9b-45948862f8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "ae = ae_model.encoder.to(device).eval()\n",
    "tpl = tpl_model.encoder.to(device).eval()\n",
    "clf = clf_model.encoder.to(device).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e3047e-4569-416c-889d-ae78045ea14d",
   "metadata": {},
   "source": [
    "### Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de11985-9985-4b0e-a56a-d75bc810e099",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_f = lambda: param.image(256 * 2, batch=2)\n",
    "obj = objectives.channel(\"avgpool\", max_channel_ae, batch=1) - objectives.channel(\"avgpool\", max_channel_ae, batch=0)\n",
    "ae_activations = render.render_vis(ae, obj, param_f, fixed_image_size=256, thresholds=(1024,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d05d2ff-0090-4393-a045-ed3a2e009474",
   "metadata": {},
   "source": [
    "### Triplet network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ba599c-6e3e-427f-ba47-678b3f2e4084",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_f = lambda: param.image(256*2, batch=2)\n",
    "obj = objectives.channel(\"8\", max_channel_tpl, batch=1) - objectives.channel(\"8\", max_channel_tpl, batch=0)\n",
    "tpl_activations = render.render_vis(tpl, obj, param_f, fixed_image_size=256, thresholds=(1024,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01ba5f0-2c55-48a0-8ac7-b16d731ffbc0",
   "metadata": {},
   "source": [
    "### Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b0bd5c-4b7a-412b-b2ae-4dac67aec008",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_f = lambda: param.image(256*2, batch=2)\n",
    "obj = objectives.channel(\"8\", max_channel_clf, batch=1) - objectives.channel(\"8\", max_channel_clf, batch=0)\n",
    "clf_activations = render.render_vis(clf, obj, param_f, fixed_image_size=256, thresholds=(1024,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031a7e54-b926-4c97-9c48-a499bada130c",
   "metadata": {},
   "source": [
    "## Compare with example images\n",
    "\n",
    "We'll now create a figure comparing images that produce a spectrum of activation values to the optimised activations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c91417-abfa-4435-be94-fe03a287b305",
   "metadata": {},
   "source": [
    "### Load paths to images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a13b8b-3909-4bd9-beff-591b49ac945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_labels = pd.read_csv(\"../output/resnet18_size-256_ae/version_6/feature-labels_train.csv\")\n",
    "tpl_labels = pd.read_csv(\"../output/resnet18_size-256_tpl/version_6/feature-labels_train.csv\")\n",
    "clf_labels = pd.read_csv(\"../output/resnet18_size-256_clf/version_17/feature-labels_train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bb5fd2-4b00-4f51-a377-22d811ef8627",
   "metadata": {},
   "source": [
    "### Utility functions\n",
    "\n",
    "These will make loading, resizing, and plotting the images easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb105d8-82ec-472e-9140-7bb2f241f73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cropresize(img, output_size, centered=True):\n",
    "    \"\"\" Crop an image to the desired aspect ratio then resize.\n",
    "    \"\"\"\n",
    "    if centered:\n",
    "        crop_pct = (0.5, 0.5)\n",
    "    else:\n",
    "        crop_pct = np.random.rand(size=2)\n",
    "        \n",
    "    if isinstance(output_size, int):\n",
    "        output_size = (output_size, output_size)\n",
    "        \n",
    "    h, w = img.shape[-2:]\n",
    "    hs, ws = output_size\n",
    "    h_pct, w_pct = crop_pct\n",
    "\n",
    "    scale = w / ws if w/ws < h/hs else h / hs\n",
    "    crop_size = (int(scale * hs), int(scale * ws))\n",
    "    \n",
    "    top, left = int(h_pct * (h - crop_size[0])), int(w_pct * (w - crop_size[1]))\n",
    "    bottom, right = crop_size[0], crop_size[1]\n",
    "    \n",
    "    return img.crop((left, top, right, bottom)).resize((hs, ws))\n",
    "\n",
    "def make_grid(paths, ids, size=256, pad=8, nrows=3, ncols=3):\n",
    "    \"\"\"Load images and make them into a grid that takes up a single numpy array.\n",
    "    \"\"\"\n",
    "    grid_height = size * nrows\n",
    "    grid_width = size * ncols\n",
    "    grid = np.ones((grid_height, grid_width, 3)).astype(int) * 255\n",
    "    \n",
    "    padded_size = size - pad\n",
    "    for i, idx in enumerate(ids):\n",
    "        img = Image.open(paths[idx])\n",
    "        img = cropresize(img, padded_size)\n",
    "        vi = i // nrows\n",
    "        hi = i % ncols\n",
    "        vi_start = vi*size + pad // 2\n",
    "        hi_start = hi*size + pad // 2\n",
    "        vi_end = size*(vi+1) - pad // 2\n",
    "        hi_end = size*(hi+1) - pad // 2\n",
    "        grid[vi_start:vi_end, hi_start:hi_end, :] = np.array(img)\n",
    "        \n",
    "    return grid\n",
    "\n",
    "def make_activations_plot(feat_values, activations, paths, n=9, d=0.2, img_size=256, inner_pad=4, outer_pad=9):\n",
    "    \"\"\"Select images to give minimum, slightly minimum, slightly maximum, and maximum values for the chosen channel,\n",
    "    and plot them alongside the optimised activations.\n",
    "    \"\"\"\n",
    "    sorted_ids = np.argsort(feat_values)\n",
    "    max_ids, min_ids = sorted_ids[-n:], sorted_ids[:n]\n",
    "    \n",
    "    delta = (feat_values.max() - feat_values.min()) * d\n",
    "    slightly_min_ids = np.argsort(abs(feat_values - delta - feat_values.min()))[:n]\n",
    "    slightly_max_ids = np.argsort(abs(feat_values + delta - feat_values.max()))[:n]\n",
    "    \n",
    "    n_side = int(np.sqrt(n))\n",
    "    \n",
    "    grid_size = img_size * n_side\n",
    "    \n",
    "    plot_size = grid_size * 6 + outer_pad*5\n",
    "    \n",
    "    plot = np.ones((grid_size, plot_size, 3)).astype(int) * 255\n",
    "    \n",
    "    start = 0\n",
    "    end = grid_size\n",
    "    plot[:, start:end, :] = (activations[0] * 255).astype(int)\n",
    "    \n",
    "    start += grid_size + outer_pad\n",
    "    end += grid_size + outer_pad\n",
    "    plot[:, start:end, :] = make_grid(paths, min_ids, pad=inner_pad, size=img_size, nrows=n_side, ncols=n_side)\n",
    "    \n",
    "    start += grid_size + outer_pad\n",
    "    end += grid_size + outer_pad\n",
    "    plot[:, start:end, :] = make_grid(paths, slightly_min_ids, pad=inner_pad, size=img_size, nrows=n_side, ncols=n_side)\n",
    "    \n",
    "    start += grid_size + outer_pad\n",
    "    end += grid_size + outer_pad\n",
    "    plot[:, start:end, :] = make_grid(paths, slightly_max_ids, pad=inner_pad, size=img_size, nrows=n_side, ncols=n_side)\n",
    "    \n",
    "    start += grid_size + outer_pad\n",
    "    end += grid_size + outer_pad\n",
    "    plot[:, start:end, :] = make_grid(paths, max_ids, pad=inner_pad, size=img_size, nrows=n_side, ncols=n_side)\n",
    "    \n",
    "    start += grid_size + outer_pad\n",
    "    end += grid_size + outer_pad\n",
    "    plot[:, start:end, :] = (activations[1] * 255).astype(int)\n",
    "    \n",
    "    return plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e4f411-d8d6-41b1-b0bc-f88b6fa979a0",
   "metadata": {},
   "source": [
    "### Make activation plot\n",
    "\n",
    "This shows examples and activations for all 3 networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3614f965-6dd1-4da2-aa6f-6dac534b957f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(8, 4), nrows=3)\n",
    "\n",
    "plots = [\n",
    "    make_activations_plot(feat_values_ae[:, max_channel_ae], ae_activations[0], ae_labels.label.values, n=4, d=0.3, outer_pad=128),\n",
    "    make_activations_plot(feat_values_tpl[:, max_channel_tpl], tpl_activations[0], tpl_labels.label.values, n=4, outer_pad=128),\n",
    "    make_activations_plot(feat_values_clf[:, max_channel_clf], clf_activations[0], clf_labels.label.values, n=4, d=0.25, outer_pad=128),\n",
    "]\n",
    "\n",
    "labels = [\"Minimum\\noptimised\", \"Minimum\\nexamples\", \"Negative\\nexamples\", \n",
    "          \"Positive\\nexamples\", \"Maximum\\nexamples\", \"Maximum\\noptimised\"]\n",
    "letters = [\"A\", \"B\", \"C\"]\n",
    "\n",
    "vpos = 619\n",
    "\n",
    "for ax, plot, letter in zip(axes, plots, letters):\n",
    "    ax.imshow(plot)\n",
    "    ax.axvline(plot.shape[1]/2, lw=2, zorder=1000, color=\"#a9a9a9\")\n",
    "    ax.text(-0.01, 0.5, letter, transform=ax.transAxes, \n",
    "                 color=\"black\", va=\"center\", ha=\"right\")\n",
    "    ax.set_axis_off()\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    hpos = i * (512 + 128) + 256\n",
    "    axes[-1].text(hpos, vpos, label, color=\"grey\", va=\"top\", ha=\"center\")\n",
    "    \n",
    "\n",
    "fig.savefig(\"../activations.png\", dpi=600, bbox_inches=\"tight\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
